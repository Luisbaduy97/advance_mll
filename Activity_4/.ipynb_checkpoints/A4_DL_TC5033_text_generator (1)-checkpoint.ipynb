{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037e89c8",
   "metadata": {},
   "source": [
    "## TC 5033\n",
    "### Text Generation\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Activity 4: Building a Simple LSTM Text Generator using WikiText-2\n",
    "<br>\n",
    "\n",
    "\n",
    "## Team #\n",
    "#### - Elmer Payro Costilla -           A01014943\n",
    "#### - Christopher Valdez Cantú -      A01793549\n",
    "#### - José Francisco Muñoz del Ángel - A01794174\n",
    "#### - Luis José Navarrete Baduy -     A01793919\n",
    "\n",
    "<br>\n",
    "\n",
    "- Objective:\n",
    "    - Gain a fundamental understanding of Long Short-Term Memory (LSTM) networks.\n",
    "    - Develop hands-on experience with sequence data processing and text generation in PyTorch. Given the simplicity of the model, amount of data, and computer resources, the text you generate will not replace ChatGPT, and results must likely will not make a lot of sense. Its only purpose is academic and to understand the text generation using RNNs.\n",
    "    - Enhance code comprehension and documentation skills by commenting on provided starter code.\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Instructions:\n",
    "    - Code Understanding: Begin by thoroughly reading and understanding the code. Comment each section/block of the provided code to demonstrate your understanding. For this, you are encouraged to add cells with experiments to improve your understanding\n",
    "\n",
    "    - Model Overview: The starter code includes an LSTM model setup for sequence data processing. Familiarize yourself with the model architecture and its components. Once you are familiar with the provided model, feel free to change the model to experiment.\n",
    "\n",
    "    - Training Function: Implement a function to train the LSTM model on the WikiText-2 dataset. This function should feed the training data into the model and perform backpropagation. \n",
    "\n",
    "    - Text Generation Function: Create a function that accepts starting text (seed text) and a specified total number of words to generate. The function should use the trained model to generate a continuation of the input text.\n",
    "\n",
    "    - Code Commenting: Ensure that all the provided starter code is well-commented. Explain the purpose and functionality of each section, indicating your understanding.\n",
    "\n",
    "    - Submission: Submit your Jupyter Notebook with all sections completed and commented. Include a markdown cell with the full names of all contributing team members at the beginning of the notebook.\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Evaluation Criteria:\n",
    "    - Code Commenting (60%): The clarity, accuracy, and thoroughness of comments explaining the provided code. You are suggested to use markdown cells for your explanations.\n",
    "\n",
    "    - Training Function Implementation (20%): The correct implementation of the training function, which should effectively train the model.\n",
    "\n",
    "    - Text Generation Functionality (10%): A working function is provided in comments. You are free to use it as long as you make sure to uderstand it, you may as well improve it as you see fit. The minimum expected is to provide comments for the given function. \n",
    "\n",
    "    - Conclusions (10%): Provide some final remarks specifying the differences you notice between this model and the one used  for classification tasks. Also comment on changes you made to the model, hyperparameters, and any other information you consider relevant. Also, please provide 3 examples of generated texts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f3333-f7d2-4b9a-92ad-7878d1f3205a",
   "metadata": {},
   "source": [
    "## Imports and Setup\n",
    "\n",
    "This section includes necessary library imports and sets up the computing device (GPU or CPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eb4b117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#PyTorch libraries\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import WikiText2\n",
    "# Dataloader library\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "# Libraries to prepare the data\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# neural layers\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8ff971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55efdd3-3ef8-4cc8-9e02-76c33f22abb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "Here, we load and process the WikiText-2 dataset for training, including tokenization and creating data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3288ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = WikiText2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc4c7dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokeniser = get_tokenizer('basic_english')\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        yield tokeniser(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c2cb068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the vocabulary\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "#set unknown token at position 0\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "134b832b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "def data_process(raw_text_iter, seq_length = 50):\n",
    "    data = [torch.tensor(vocab(tokeniser(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data))) #remove empty tensors\n",
    "#     target_data = torch.cat(d)\n",
    "    return (data[:-(data.size(0)%seq_length)].view(-1, seq_length), \n",
    "            data[1:-(data.size(0)%seq_length-1)].view(-1, seq_length))  \n",
    "\n",
    "# # Create tensors for the training set\n",
    "x_train, y_train = data_process(train_dataset, seq_length)\n",
    "x_val, y_val = data_process(val_dataset, seq_length)\n",
    "x_test, y_test = data_process(test_dataset, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b54c04d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create TensorDataset objects for DataLoader\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d400fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64  # choose a batch size that fits your computation resources\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056038a0-75cf-42f9-ada6-34b8fe45bca7",
   "metadata": {},
   "source": [
    "## LSTM Model\n",
    "\n",
    "Defines an enhanced LSTM model with additional layers and dropout for better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59c63b01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "# Feel free to experiment\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, text, hidden):\n",
    "        embeddings = self.embeddings(text)\n",
    "        output, hidden = self.lstm(embeddings, hidden)\n",
    "        decoded = self.fc(output)\n",
    "        return decoded, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = len(vocab) # vocabulary size\n",
    "emb_size = 100 # embedding size\n",
    "neurons = 128 # the dimension of the feedforward network model, i.e. # of neurons \n",
    "num_layers = 1 # the number of nn.LSTM layers\n",
    "model = LSTMModel(vocab_size, emb_size, neurons, num_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9beafea-d68a-404a-9cb7-4162feb9394f",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "\n",
    "Enhanced training function with gradient clipping and other adjustments for improved model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "215eabb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, optimiser):\n",
    "    '''\n",
    "    The following are possible instructions you may want to conside for this function.\n",
    "    This is only a guide and you may change add or remove whatever you consider appropriate\n",
    "    as long as you train your model correctly.\n",
    "        - loop through specified epochs\n",
    "        - loop through dataloader\n",
    "        - don't forget to zero grad!\n",
    "        - place data (both input and target) in device\n",
    "        - init hidden states e.g. hidden = model.init_hidden(batch_size)\n",
    "        - run the model\n",
    "        - compute the cost or loss\n",
    "        - backpropagation\n",
    "        - Update paratemers\n",
    "        - Include print all the information you consider helpful\n",
    "    \n",
    "    '''\n",
    "    model = model.to(device=device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (data, targets) in enumerate((train_loader)):\n",
    "            optimiser.zero_grad()  # Zero out the gradients\n",
    "            data, targets = data.to(device), targets.to(device)  # Move data to the device\n",
    "            hidden = model.init_hidden(batch_size)  # Initialize hidden states\n",
    "            output, _ = model(data, hidden)  # Forward pass\n",
    "            loss = loss_function(output.view(-1, vocab_size), targets.view(-1))  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimiser.step()  # Update model parameters\n",
    "\n",
    "            if i % 100 == 0:  # Optionally print the loss every 100 batches\n",
    "                print(f\"Epoch {epoch}, Batch {i}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275d4419-cda3-46be-9ba2-a069405bdb0b",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "Function to generate text using the trained model, with temperature control for randomness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2405e9bf-580d-4fda-b5cb-93a3bdf68aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_text, num_words, temperature=1.0):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    words = tokeniser(start_text)\n",
    "    hidden = model.init_hidden(1)\n",
    "    for i in range(0, num_words):\n",
    "        x = torch.tensor([[vocab[word] for word in words[i:]]], dtype=torch.long, device=device)\n",
    "        y_pred, hidden = model(x, hidden)\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = F.softmax(last_word_logits / temperature, dim=0).detach().to(device='cpu').numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(vocab.lookup_token(word_index))\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae8ad5-6784-409a-a087-4f99bf56b3d5",
   "metadata": {},
   "source": [
    "## Training and Generating Text\n",
    "\n",
    "Train the model with the new settings and then use it to generate text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7bd5f24-c72f-442a-9f7e-1a31d81b0182",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Chris\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Chris\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 0, Batch 0, Loss: 10.26456069946289\n",
      "Epoch 0, Batch 100, Loss: 7.1247100830078125\n",
      "Epoch 0, Batch 200, Loss: 7.0313849449157715\n",
      "Epoch 0, Batch 300, Loss: 6.931764602661133\n",
      "Epoch 0, Batch 400, Loss: 6.799363613128662\n",
      "Epoch 0, Batch 500, Loss: 6.76700496673584\n",
      "Epoch 0, Batch 600, Loss: 6.647710800170898\n",
      "Epoch 1, Batch 0, Loss: 6.6066694259643555\n",
      "Epoch 1, Batch 100, Loss: 6.551435947418213\n",
      "Epoch 1, Batch 200, Loss: 6.542983531951904\n",
      "Epoch 1, Batch 300, Loss: 6.4150590896606445\n",
      "Epoch 1, Batch 400, Loss: 6.366401195526123\n",
      "Epoch 1, Batch 500, Loss: 6.314639091491699\n",
      "Epoch 1, Batch 600, Loss: 6.281449794769287\n",
      "Epoch 2, Batch 0, Loss: 6.346937656402588\n",
      "Epoch 2, Batch 100, Loss: 6.321228504180908\n",
      "Epoch 2, Batch 200, Loss: 6.297708034515381\n",
      "Epoch 2, Batch 300, Loss: 6.179828643798828\n",
      "Epoch 2, Batch 400, Loss: 6.162801742553711\n",
      "Epoch 2, Batch 500, Loss: 6.199451446533203\n",
      "Epoch 2, Batch 600, Loss: 6.054376602172852\n",
      "Epoch 3, Batch 0, Loss: 6.167628288269043\n",
      "Epoch 3, Batch 100, Loss: 6.151877403259277\n",
      "Epoch 3, Batch 200, Loss: 5.941867828369141\n",
      "Epoch 3, Batch 300, Loss: 6.029512405395508\n",
      "Epoch 3, Batch 400, Loss: 6.005789756774902\n",
      "Epoch 3, Batch 500, Loss: 6.094986438751221\n",
      "Epoch 3, Batch 600, Loss: 6.009413242340088\n",
      "Epoch 4, Batch 0, Loss: 5.7837347984313965\n",
      "Epoch 4, Batch 100, Loss: 5.866991996765137\n",
      "Epoch 4, Batch 200, Loss: 5.799745082855225\n",
      "Epoch 4, Batch 300, Loss: 5.830196380615234\n",
      "Epoch 4, Batch 400, Loss: 5.922910690307617\n",
      "Epoch 4, Batch 500, Loss: 5.950567722320557\n",
      "Epoch 4, Batch 600, Loss: 5.7098822593688965\n",
      "Epoch 5, Batch 0, Loss: 5.869802474975586\n",
      "Epoch 5, Batch 100, Loss: 5.890683174133301\n",
      "Epoch 5, Batch 200, Loss: 5.880949020385742\n",
      "Epoch 5, Batch 300, Loss: 5.780149936676025\n",
      "Epoch 5, Batch 400, Loss: 5.750082492828369\n",
      "Epoch 5, Batch 500, Loss: 5.853369235992432\n",
      "Epoch 5, Batch 600, Loss: 5.7387375831604\n",
      "Epoch 6, Batch 0, Loss: 5.66770076751709\n",
      "Epoch 6, Batch 100, Loss: 5.663785457611084\n",
      "Epoch 6, Batch 200, Loss: 5.5736613273620605\n",
      "Epoch 6, Batch 300, Loss: 5.700178623199463\n",
      "Epoch 6, Batch 400, Loss: 5.743992328643799\n",
      "Epoch 6, Batch 500, Loss: 5.829639911651611\n",
      "Epoch 6, Batch 600, Loss: 5.645557403564453\n",
      "Epoch 7, Batch 0, Loss: 5.554847240447998\n",
      "Epoch 7, Batch 100, Loss: 5.715068340301514\n",
      "Epoch 7, Batch 200, Loss: 5.737236499786377\n",
      "Epoch 7, Batch 300, Loss: 5.569799900054932\n",
      "Epoch 7, Batch 400, Loss: 5.663558483123779\n",
      "Epoch 7, Batch 500, Loss: 5.6305251121521\n",
      "Epoch 7, Batch 600, Loss: 5.596466541290283\n",
      "Epoch 8, Batch 0, Loss: 5.495136260986328\n",
      "Epoch 8, Batch 100, Loss: 5.4057111740112305\n",
      "Epoch 8, Batch 200, Loss: 5.528403282165527\n",
      "Epoch 8, Batch 300, Loss: 5.510059356689453\n",
      "Epoch 8, Batch 400, Loss: 5.530395984649658\n",
      "Epoch 8, Batch 500, Loss: 5.56287956237793\n",
      "Epoch 8, Batch 600, Loss: 5.548922061920166\n",
      "Epoch 9, Batch 0, Loss: 5.561470985412598\n",
      "Epoch 9, Batch 100, Loss: 5.407287120819092\n",
      "Epoch 9, Batch 200, Loss: 5.563410758972168\n",
      "Epoch 9, Batch 300, Loss: 5.4193243980407715\n",
      "Epoch 9, Batch 400, Loss: 5.466723442077637\n",
      "Epoch 9, Batch 500, Loss: 5.489290237426758\n",
      "Epoch 9, Batch 600, Loss: 5.50196647644043\n",
      "Epoch 10, Batch 0, Loss: 5.497351169586182\n",
      "Epoch 10, Batch 100, Loss: 5.493320941925049\n",
      "Epoch 10, Batch 200, Loss: 5.526696681976318\n",
      "Epoch 10, Batch 300, Loss: 5.386681079864502\n",
      "Epoch 10, Batch 400, Loss: 5.4603352546691895\n",
      "Epoch 10, Batch 500, Loss: 5.384750843048096\n",
      "Epoch 10, Batch 600, Loss: 5.382599353790283\n",
      "Epoch 11, Batch 0, Loss: 5.431755542755127\n",
      "Epoch 11, Batch 100, Loss: 5.3127522468566895\n",
      "Epoch 11, Batch 200, Loss: 5.57198429107666\n",
      "Epoch 11, Batch 300, Loss: 5.324751377105713\n",
      "Epoch 11, Batch 400, Loss: 5.380293369293213\n",
      "Epoch 11, Batch 500, Loss: 5.239541053771973\n",
      "Epoch 11, Batch 600, Loss: 5.46084451675415\n",
      "Epoch 12, Batch 0, Loss: 5.280871391296387\n",
      "Epoch 12, Batch 100, Loss: 5.488772392272949\n",
      "Epoch 12, Batch 200, Loss: 5.288707256317139\n",
      "Epoch 12, Batch 300, Loss: 5.468568801879883\n",
      "Epoch 12, Batch 400, Loss: 5.429042816162109\n",
      "Epoch 12, Batch 500, Loss: 5.388390064239502\n",
      "Epoch 12, Batch 600, Loss: 5.512859344482422\n",
      "Epoch 13, Batch 0, Loss: 5.296133995056152\n",
      "Epoch 13, Batch 100, Loss: 5.386018753051758\n",
      "Epoch 13, Batch 200, Loss: 5.2154998779296875\n",
      "Epoch 13, Batch 300, Loss: 5.1490159034729\n",
      "Epoch 13, Batch 400, Loss: 5.371848106384277\n",
      "Epoch 13, Batch 500, Loss: 5.440516471862793\n",
      "Epoch 13, Batch 600, Loss: 5.251065254211426\n",
      "Epoch 14, Batch 0, Loss: 5.267874240875244\n",
      "Epoch 14, Batch 100, Loss: 5.205519199371338\n",
      "Epoch 14, Batch 200, Loss: 5.281148910522461\n",
      "Epoch 14, Batch 300, Loss: 5.341528415679932\n",
      "Epoch 14, Batch 400, Loss: 5.206837177276611\n",
      "Epoch 14, Batch 500, Loss: 5.380432605743408\n",
      "Epoch 14, Batch 600, Loss: 5.195212364196777\n",
      "Epoch 15, Batch 0, Loss: 5.226614952087402\n",
      "Epoch 15, Batch 100, Loss: 5.265405654907227\n",
      "Epoch 15, Batch 200, Loss: 5.141067028045654\n",
      "Epoch 15, Batch 300, Loss: 5.225668430328369\n",
      "Epoch 15, Batch 400, Loss: 5.243283271789551\n",
      "Epoch 15, Batch 500, Loss: 5.29118537902832\n",
      "Epoch 15, Batch 600, Loss: 5.2287187576293945\n",
      "Epoch 16, Batch 0, Loss: 5.27829122543335\n",
      "Epoch 16, Batch 100, Loss: 5.1741156578063965\n",
      "Epoch 16, Batch 200, Loss: 5.235747814178467\n",
      "Epoch 16, Batch 300, Loss: 5.134048938751221\n",
      "Epoch 16, Batch 400, Loss: 5.2503342628479\n",
      "Epoch 16, Batch 500, Loss: 5.207944869995117\n",
      "Epoch 16, Batch 600, Loss: 5.209765434265137\n",
      "Epoch 17, Batch 0, Loss: 5.155480861663818\n",
      "Epoch 17, Batch 100, Loss: 5.096780776977539\n",
      "Epoch 17, Batch 200, Loss: 5.135497570037842\n",
      "Epoch 17, Batch 300, Loss: 5.200878143310547\n",
      "Epoch 17, Batch 400, Loss: 5.029346466064453\n",
      "Epoch 17, Batch 500, Loss: 5.182489395141602\n",
      "Epoch 17, Batch 600, Loss: 5.132435321807861\n",
      "Epoch 18, Batch 0, Loss: 5.039045333862305\n",
      "Epoch 18, Batch 100, Loss: 5.034605979919434\n",
      "Epoch 18, Batch 200, Loss: 5.097629547119141\n",
      "Epoch 18, Batch 300, Loss: 5.083559036254883\n",
      "Epoch 18, Batch 400, Loss: 5.193403244018555\n",
      "Epoch 18, Batch 500, Loss: 5.230874538421631\n",
      "Epoch 18, Batch 600, Loss: 5.096179962158203\n",
      "Epoch 19, Batch 0, Loss: 5.09254789352417\n",
      "Epoch 19, Batch 100, Loss: 5.084226608276367\n",
      "Epoch 19, Batch 200, Loss: 5.168116569519043\n",
      "Epoch 19, Batch 300, Loss: 5.046704292297363\n",
      "Epoch 19, Batch 400, Loss: 5.0969743728637695\n",
      "Epoch 19, Batch 500, Loss: 5.011875152587891\n",
      "Epoch 19, Batch 600, Loss: 5.229834079742432\n"
     ]
    }
   ],
   "source": [
    "# Call the train function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "lr = 0.0005\n",
    "epochs = 20\n",
    "optimiser = optim.Adam(model.parameters(), lr=lr)\n",
    "train(model, epochs, optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2884543",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like ernest and traps and were relegated to record spain alleged to solve do not have resembled armour <unk> , loutit , <unk> , and a new defeat . the ioc new york — fu ! <unk> amos . its overwhelmingly you work read , why the mystery of <unk> in the arkansas journal of <unk> , juno , from the mystery in most quarters , it would not yet captured the selenites . she ' pair , succeeded robyn ( <unk> ) to shining its replacement . by 19th @-@ century @-@ ray configuration chauvel stood on to release and\n"
     ]
    }
   ],
   "source": [
    "# Generate some text\n",
    "print(generate_text(model, start_text=\"I like\", num_words=100), \"\\n\")\n",
    "print(generate_text(model, start_text=\"I like\", num_words=100), \"\\n\")\n",
    "print(generate_text(model, start_text=\"I like\", num_words=100), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0a87ee-2d5b-4585-a12d-bc70e765ae10",
   "metadata": {},
   "source": [
    "## Enhanced LSTM Model\n",
    "\n",
    "This part of the code defines an improved version of the LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a16ca9c-815b-4d0f-97a7-442a482622f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EnhancedLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.5):\n",
    "        super(EnhancedLSTMModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # Adding a second LSTM layer and dropout for regularization\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, text, hidden):\n",
    "        embeddings = self.embeddings(text)\n",
    "        output, hidden = self.lstm(embeddings, hidden)\n",
    "        decoded = self.fc(output)\n",
    "        return decoded, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
    "\n",
    "# Update model instantiation\n",
    "neurons = 256  # Increased number of neurons\n",
    "num_layers = 2  # Increased number of layers\n",
    "dropout = 0.3  # Dropout for regularization\n",
    "model = EnhancedLSTMModel(vocab_size, emb_size, neurons, num_layers, dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78eabe9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 10.262259483337402\n",
      "Epoch 0, Batch 100, Loss: 7.00984001159668\n",
      "Epoch 0, Batch 200, Loss: 7.085801601409912\n",
      "Epoch 0, Batch 300, Loss: 6.961468696594238\n",
      "Epoch 0, Batch 400, Loss: 6.92379093170166\n",
      "Epoch 0, Batch 500, Loss: 6.877636909484863\n",
      "Epoch 0, Batch 600, Loss: 6.566391468048096\n",
      "Epoch 1, Batch 0, Loss: 6.695781230926514\n",
      "Epoch 1, Batch 100, Loss: 6.5917582511901855\n",
      "Epoch 1, Batch 200, Loss: 6.489748954772949\n",
      "Epoch 1, Batch 300, Loss: 6.432582855224609\n",
      "Epoch 1, Batch 400, Loss: 6.377783298492432\n",
      "Epoch 1, Batch 500, Loss: 6.251681327819824\n",
      "Epoch 1, Batch 600, Loss: 6.366598129272461\n",
      "Epoch 2, Batch 0, Loss: 6.329485893249512\n",
      "Epoch 2, Batch 100, Loss: 6.253326416015625\n",
      "Epoch 2, Batch 200, Loss: 6.327491283416748\n",
      "Epoch 2, Batch 300, Loss: 6.215903282165527\n",
      "Epoch 2, Batch 400, Loss: 6.119957447052002\n",
      "Epoch 2, Batch 500, Loss: 6.150346755981445\n",
      "Epoch 2, Batch 600, Loss: 6.175166606903076\n",
      "Epoch 3, Batch 0, Loss: 5.963206768035889\n",
      "Epoch 3, Batch 100, Loss: 6.0550994873046875\n",
      "Epoch 3, Batch 200, Loss: 5.949321746826172\n",
      "Epoch 3, Batch 300, Loss: 5.968708038330078\n",
      "Epoch 3, Batch 400, Loss: 5.958139419555664\n",
      "Epoch 3, Batch 500, Loss: 6.038639068603516\n",
      "Epoch 3, Batch 600, Loss: 5.923176288604736\n",
      "Epoch 4, Batch 0, Loss: 5.902453422546387\n",
      "Epoch 4, Batch 100, Loss: 5.889980316162109\n",
      "Epoch 4, Batch 200, Loss: 5.932281017303467\n",
      "Epoch 4, Batch 300, Loss: 5.797879219055176\n",
      "Epoch 4, Batch 400, Loss: 5.952865123748779\n",
      "Epoch 4, Batch 500, Loss: 5.736511707305908\n",
      "Epoch 4, Batch 600, Loss: 5.807060718536377\n",
      "Epoch 5, Batch 0, Loss: 5.848184108734131\n",
      "Epoch 5, Batch 100, Loss: 5.769415378570557\n",
      "Epoch 5, Batch 200, Loss: 5.806941986083984\n",
      "Epoch 5, Batch 300, Loss: 5.693695545196533\n",
      "Epoch 5, Batch 400, Loss: 5.6739888191223145\n",
      "Epoch 5, Batch 500, Loss: 5.634567737579346\n",
      "Epoch 5, Batch 600, Loss: 5.701640605926514\n",
      "Epoch 6, Batch 0, Loss: 5.627603054046631\n",
      "Epoch 6, Batch 100, Loss: 5.7053022384643555\n",
      "Epoch 6, Batch 200, Loss: 5.605166435241699\n",
      "Epoch 6, Batch 300, Loss: 5.670582294464111\n",
      "Epoch 6, Batch 400, Loss: 5.620718479156494\n",
      "Epoch 6, Batch 500, Loss: 5.590358257293701\n",
      "Epoch 6, Batch 600, Loss: 5.542242527008057\n",
      "Epoch 7, Batch 0, Loss: 5.499162673950195\n",
      "Epoch 7, Batch 100, Loss: 5.587557315826416\n",
      "Epoch 7, Batch 200, Loss: 5.613739013671875\n",
      "Epoch 7, Batch 300, Loss: 5.525787353515625\n",
      "Epoch 7, Batch 400, Loss: 5.268218994140625\n",
      "Epoch 7, Batch 500, Loss: 5.479731559753418\n",
      "Epoch 7, Batch 600, Loss: 5.4084296226501465\n",
      "Epoch 8, Batch 0, Loss: 5.445491790771484\n",
      "Epoch 8, Batch 100, Loss: 5.480307579040527\n",
      "Epoch 8, Batch 200, Loss: 5.2461066246032715\n",
      "Epoch 8, Batch 300, Loss: 5.268362998962402\n",
      "Epoch 8, Batch 400, Loss: 5.2895588874816895\n",
      "Epoch 8, Batch 500, Loss: 5.185376167297363\n",
      "Epoch 8, Batch 600, Loss: 5.3002400398254395\n",
      "Epoch 9, Batch 0, Loss: 5.287512302398682\n",
      "Epoch 9, Batch 100, Loss: 5.21055793762207\n",
      "Epoch 9, Batch 200, Loss: 5.1933698654174805\n",
      "Epoch 9, Batch 300, Loss: 5.227096080780029\n",
      "Epoch 9, Batch 400, Loss: 5.1520819664001465\n",
      "Epoch 9, Batch 500, Loss: 5.180822849273682\n",
      "Epoch 9, Batch 600, Loss: 5.382150650024414\n",
      "Epoch 10, Batch 0, Loss: 5.074459075927734\n",
      "Epoch 10, Batch 100, Loss: 5.127035617828369\n",
      "Epoch 10, Batch 200, Loss: 5.103496551513672\n",
      "Epoch 10, Batch 300, Loss: 5.144326210021973\n",
      "Epoch 10, Batch 400, Loss: 5.266181468963623\n",
      "Epoch 10, Batch 500, Loss: 4.932818412780762\n",
      "Epoch 10, Batch 600, Loss: 5.173951625823975\n",
      "Epoch 11, Batch 0, Loss: 5.03122091293335\n",
      "Epoch 11, Batch 100, Loss: 5.011163711547852\n",
      "Epoch 11, Batch 200, Loss: 4.913970470428467\n",
      "Epoch 11, Batch 300, Loss: 5.029000759124756\n",
      "Epoch 11, Batch 400, Loss: 5.001174449920654\n",
      "Epoch 11, Batch 500, Loss: 4.909159183502197\n",
      "Epoch 11, Batch 600, Loss: 4.9156951904296875\n",
      "Epoch 12, Batch 0, Loss: 4.981960296630859\n",
      "Epoch 12, Batch 100, Loss: 4.962615489959717\n",
      "Epoch 12, Batch 200, Loss: 4.97200345993042\n",
      "Epoch 12, Batch 300, Loss: 5.004245758056641\n",
      "Epoch 12, Batch 400, Loss: 4.8636155128479\n",
      "Epoch 12, Batch 500, Loss: 4.8445868492126465\n",
      "Epoch 12, Batch 600, Loss: 4.88478946685791\n",
      "Epoch 13, Batch 0, Loss: 4.795712471008301\n",
      "Epoch 13, Batch 100, Loss: 4.785953521728516\n",
      "Epoch 13, Batch 200, Loss: 4.756042003631592\n",
      "Epoch 13, Batch 300, Loss: 4.904345989227295\n",
      "Epoch 13, Batch 400, Loss: 4.941287040710449\n",
      "Epoch 13, Batch 500, Loss: 5.015562534332275\n",
      "Epoch 13, Batch 600, Loss: 4.875340938568115\n",
      "Epoch 14, Batch 0, Loss: 4.796106338500977\n",
      "Epoch 14, Batch 100, Loss: 4.755903244018555\n",
      "Epoch 14, Batch 200, Loss: 4.736389636993408\n",
      "Epoch 14, Batch 300, Loss: 4.72951602935791\n",
      "Epoch 14, Batch 400, Loss: 4.756868362426758\n",
      "Epoch 14, Batch 500, Loss: 4.686063766479492\n",
      "Epoch 14, Batch 600, Loss: 4.7665252685546875\n",
      "Epoch 15, Batch 0, Loss: 4.633125305175781\n",
      "Epoch 15, Batch 100, Loss: 4.671525955200195\n",
      "Epoch 15, Batch 200, Loss: 4.699465751647949\n",
      "Epoch 15, Batch 300, Loss: 4.784615993499756\n",
      "Epoch 15, Batch 400, Loss: 4.6966681480407715\n",
      "Epoch 15, Batch 500, Loss: 4.709494590759277\n",
      "Epoch 15, Batch 600, Loss: 4.668466091156006\n",
      "Epoch 16, Batch 0, Loss: 4.6613850593566895\n",
      "Epoch 16, Batch 100, Loss: 4.597907066345215\n",
      "Epoch 16, Batch 200, Loss: 4.619679927825928\n",
      "Epoch 16, Batch 300, Loss: 4.563015937805176\n",
      "Epoch 16, Batch 400, Loss: 4.66085958480835\n",
      "Epoch 16, Batch 500, Loss: 4.6787004470825195\n",
      "Epoch 16, Batch 600, Loss: 4.623655796051025\n",
      "Epoch 17, Batch 0, Loss: 4.4706196784973145\n",
      "Epoch 17, Batch 100, Loss: 4.565463066101074\n",
      "Epoch 17, Batch 200, Loss: 4.502819061279297\n",
      "Epoch 17, Batch 300, Loss: 4.829955577850342\n",
      "Epoch 17, Batch 400, Loss: 4.52459716796875\n",
      "Epoch 17, Batch 500, Loss: 4.712395191192627\n",
      "Epoch 17, Batch 600, Loss: 4.549072265625\n",
      "Epoch 18, Batch 0, Loss: 4.5272536277771\n",
      "Epoch 18, Batch 100, Loss: 4.510489463806152\n",
      "Epoch 18, Batch 200, Loss: 4.434931755065918\n",
      "Epoch 18, Batch 300, Loss: 4.65226936340332\n",
      "Epoch 18, Batch 400, Loss: 4.619894981384277\n",
      "Epoch 18, Batch 500, Loss: 4.560058116912842\n",
      "Epoch 18, Batch 600, Loss: 4.535378932952881\n",
      "Epoch 19, Batch 0, Loss: 4.385106086730957\n",
      "Epoch 19, Batch 100, Loss: 4.457848072052002\n",
      "Epoch 19, Batch 200, Loss: 4.475701808929443\n",
      "Epoch 19, Batch 300, Loss: 4.462334632873535\n",
      "Epoch 19, Batch 400, Loss: 4.510268688201904\n",
      "Epoch 19, Batch 500, Loss: 4.467714786529541\n",
      "Epoch 19, Batch 600, Loss: 4.5252556800842285\n"
     ]
    }
   ],
   "source": [
    "def enhanced_train(model, epochs, optimiser, clip=1):\n",
    "    model = model.to(device=device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, (data, targets) in enumerate((train_loader)):\n",
    "            optimiser.zero_grad()\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "            output, _ = model(data, hidden)\n",
    "            loss = loss_function(output.view(-1, vocab_size), targets.view(-1))\n",
    "            loss.backward()\n",
    "            # Implement gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimiser.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Batch {i}, Loss: {loss.item()}\")\n",
    "\n",
    "# Adjust the learning rate\n",
    "lr = 0.001  # Slightly increased learning rate\n",
    "epochs = 20  # More training epochs\n",
    "optimiser = optim.Adam(model.parameters(), lr=lr)\n",
    "enhanced_train(model, epochs, optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cb126a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_text, num_words, temperature=1.0):\n",
    "    model.eval()\n",
    "    words = tokeniser(start_text)\n",
    "    hidden = model.init_hidden(1)\n",
    "    for i in range(0, num_words):\n",
    "        x = torch.tensor([[vocab[word] for word in words[i:]]], dtype=torch.long, device=device)\n",
    "        y_pred, hidden = model(x, hidden)\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = F.softmax(last_word_logits / temperature, dim=0).detach().to(device='cpu').numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(vocab.lookup_token(word_index))\n",
    "\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6de373d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like . ( macpherson ) is a synonym and found it is not . the same , robert copsey it seems to have been unlikely to do her 59 , which later explain some thoughts sora makes sit on ohkawa ' s everything ? [ confessions ] . at half development has been recorded well of the same stanza of christ . thus wica and the part of the school wolf , and alison life , in the production ’ s and symphony . the nike meant a <unk> release . after dare to deliver <unk> readings on them in both \n",
      "\n",
      "i like iv and tossed the food group , were debated on the front corner together for him missed the ball atmosphere . despite the most <unk> for each failure indicated that the genre should be north of the front of the 1998 dollar in both to brazil and james <unk> of madame doug joey , with a guitarist , is among a bird , with the last tactic of a group . she was the name of cilicia , after the park had spent three years before his election sold in the top six years due to counter texas . = \n",
      "\n",
      "i like young 45 – 10 ! ? 9 – g ] 33 years southwest . = = visual and unchanging age = = judith also placed his playing pieces by steven james ' s an actor who gives it a small life , and gives comment for the tv video genre . <unk> rosen masks that white joins the fear of a background young against mr . phillip , working the other metal mrs parks ( a track # 27 – 37 ° f . cdt , ) is a term lancet to 1 @ . @ @-@ 71 % of \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_text=\"I like\", num_words=100), \"\\n\")\n",
    "print(generate_text(model, start_text=\"I like\", num_words=100), \"\\n\")\n",
    "print(generate_text(model, start_text=\"I like\", num_words=100), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d82438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

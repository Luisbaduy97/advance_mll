{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TC 5033\n",
    "## Deep Learning\n",
    "## Fully Connected Deep Neural Networks\n",
    "\n",
    "## Team #\n",
    "#### * Elmer Payro Costilla -           A01014943\n",
    "#### * Christopher Valdez Cantú -      A01793549\n",
    "#### * José Francisco Muñoz del Ángel - A01794174\n",
    "#### * Luis José Navarrete Baduy -     A01793919\n",
    "\n",
    "## Activity 1b: Implementing a Fully Connected Network for Kaggle ASL Dataset\n",
    "\n",
    "- Objective\n",
    "\n",
    "The aim of this part of the activity is to apply your understanding of Fully Connected Networks by implementing a multilayer network for the [Kaggle ASL (American Sign Language) dataset](https://www.kaggle.com/datasets/grassknoted/asl-alphabet). While you have been provided with a complete solution for a Fully Connected Network using Numpy for the MNIST dataset, you are encouraged to try to come up with the solution.\n",
    "\n",
    "- Instructions\n",
    "\n",
    "    This activity requires submission in teams of 3 or 4 members. Submissions from smaller or larger teams will not be accepted unless prior approval has been granted (only due to exceptional circumstances). While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
    "\n",
    "    Load and Preprocess Data: You are provided a starter code to load the data. Be sure to understand the code.\n",
    "\n",
    "    Review MNIST Notebook (Optional): Before diving into this activity, you have the option to revisit the MNIST example to refresh your understanding of how to build a Fully Connected Network using Numpy.\n",
    "\n",
    "    Start Fresh: Although you can refer to the MNIST solution at any point, try to implement the network for the ASL dataset on your own. This will reinforce your learning and understanding of the architecture and mathematics involved.\n",
    "\n",
    "    Implement Forward and Backward Pass: Write the code to perform the forward and backward passes, keeping in mind the specific challenges and characteristics of the ASL dataset.\n",
    "    \n",
    "     Design the Network: Create the architecture of the Fully Connected Network tailored for the ASL dataset. Choose the number of hidden layers, neurons, and hyperparameters judiciously.\n",
    "\n",
    "    Train the Model: Execute the training loop, ensuring to track performance metrics such as loss and accuracy.\n",
    "\n",
    "    Analyze and Document: Use Markdown cells to document in detail the choices you made in terms of architecture and hyperparameters, you may use figures, equations, etc to aid in your explanations. Include any metrics that help justify these choices and discuss the model's performance.  \n",
    "\n",
    "- Evaluation Criteria\n",
    "\n",
    "    - Code Readability and Comments\n",
    "    - Appropriateness of chosen architecture and hyperparameters for the ASL dataset\n",
    "    - Performance of the model on the ASL dataset (at least 70% acc)\n",
    "    - Quality of Markdown documentation\n",
    "\n",
    "- Submission\n",
    "\n",
    "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import cv2 as cv\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#################################\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the directory containing the CSV data files\n",
    "DATA_PATH = 'asl_data'\n",
    "\n",
    "# Load the training data from the 'sign_mnist_train.csv' CSV file\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_train.csv'))\n",
    "\n",
    "# Load the validation data from the 'sign_mnist_valid.csv' CSV file\n",
    "valid_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_valid.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      3     107     118     127     134     139     143     146     150   \n",
       "1      6     155     157     156     156     156     157     156     158   \n",
       "2      2     187     188     188     187     187     186     187     188   \n",
       "3      2     211     211     212     212     211     210     211     210   \n",
       "4     12     164     167     170     172     176     179     180     184   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     153  ...       207       207       207       207       206       206   \n",
       "1     158  ...        69       149       128        87        94       163   \n",
       "2     187  ...       202       201       200       199       198       199   \n",
       "3     210  ...       235       234       233       231       230       226   \n",
       "4     185  ...        92       105       105       108       133       163   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       206       204       203       202  \n",
       "1       175       103       135       149  \n",
       "2       198       195       194       195  \n",
       "3       225       222       229       163  \n",
       "4       157       163       164       179  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the training data DataFrame\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the labels for training data from the 'label' column\n",
    "y_train = np.array(train_df['label'])\n",
    "\n",
    "# Extract the labels for validation data from the 'label' column\n",
    "yv = np.array(valid_df['label'])\n",
    "\n",
    "# Extract the features for training data by removing the 'label' column and converting to NumPy array \n",
    "x_train = train_df.drop(columns=['label']).values.astype(np.float32)\n",
    "\n",
    "# Extract the features for validation data by removing the 'label' column and converting to NumPy array\n",
    "xv = valid_df.drop(columns=['label']).values.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'label' column from the training DataFrame\n",
    "del train_df['label']\n",
    "# Remove the 'label' column from the validation DataFrame\n",
    "del valid_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7172, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3586"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(xv.shape)\n",
    "round(xv.shape[0]*.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def split_val_test(x, y, pct=0.6, shuffle=False):\n",
    "    '''\n",
    "    Create a function that will allow you to split the previously loaded validation set\n",
    "    into valition and test.\n",
    "    '''\n",
    "    cutoff = round(x.shape[0] * pct)    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(x) # Shuffle the input features fixme\n",
    "        np.random.shuffle(y) # Shuffle the target labels\n",
    "    # Split the data into training and validation sets\n",
    "    return x[:cutoff],y[:cutoff],x[cutoff:],y[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the validation data (xv, yv) into validation and test sets using the split_val_test function\n",
    "x_val, y_val, x_test, y_test = split_val_test(xv, yv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4303, 784)\n",
      "(2869, 784)\n"
     ]
    }
   ],
   "source": [
    "print(x_val.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159.29083, 48.76953, 0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.mean(), x_train.std(), x_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x_mean, x_std, x_data):\n",
    "    '''\n",
    "    Normalize a dataset using mean and standard deviation.\n",
    "    \n",
    "    Parameters:\n",
    "    - x_mean: Mean of the dataset.\n",
    "    - x_std: Standard deviation of the dataset.\n",
    "    - x_data: Data to be normalized.\n",
    "    \n",
    "    Returns:\n",
    "    - Normalized data.\n",
    "    '''\n",
    "    return (x_data - x_mean) / x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation of the training data\n",
    "x_mean = x_train.mean()\n",
    "x_std = x_train.std()\n",
    "\n",
    "# Normalize the training data using the calculated mean and standard deviation\n",
    "x_train = normalize(x_mean, x_std, x_train)\n",
    "\n",
    "# Normalize the validation data using the same mean and standard deviation as the training data\n",
    "x_val = normalize(x_mean, x_std, x_val)\n",
    "\n",
    "# Normalize the test data using the same mean and standard deviation as the training data\n",
    "x_test = normalize(x_mean, x_std, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.6268384e-06, 0.99999946)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.mean(), x_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_number(image):\n",
    "    '''\n",
    "    Plot a grayscale image.\n",
    "\n",
    "    Parameters:\n",
    "    - image: The image data to be plotted.\n",
    "\n",
    "    Displays the input image as a grayscale plot with no axis.\n",
    "    '''\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(image.reshape(28,28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La imagen muestreada representa un: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a random index within the length of the test labels (y_test)\n",
    "rnd_idx = np.random.randint(len(y_test))\n",
    "\n",
    "# Print the label of the randomly selected image from the test set\n",
    "print(f'La imagen muestreada representa un: {y_test[rnd_idx]}')\n",
    "\n",
    "# Plot the randomly selected image using the plot_number function\n",
    "plot_number(xv[rnd_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecuaciones para nuestro modelo\n",
    "\n",
    "\n",
    "$$z^1 = W^1 X + b^1$$\n",
    "\n",
    "$$a^1 = ReLU(z^1) $$\n",
    "\n",
    "$$z^2 = W^2 a^1 + b^2$$\n",
    "\n",
    "$$\\hat{y} = \\frac{e^{z^{2_k}}}{\\sum_j{e^{z_j}}}$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}^{i}, y^{i}) =  - y^{i}  \\ln(\\hat{y}^{i}) = -\\ln(\\hat{y}^i)$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{J}(w, b) =  \\frac{1}{num\\_samples} \\sum_{i=1}^{num\\_samples}-\\ln(\\hat{y}^{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones adicionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatches(mb_size, x, y, shuffle = True):\n",
    "    '''\n",
    "    Create minibatches from the input data for mini-batch training.\n",
    "\n",
    "    Parameters:\n",
    "    - mb_size: Size of each minibatch.\n",
    "    - x: Input features.\n",
    "    - y: Target labels.\n",
    "    - shuffle: Whether to shuffle the data before creating minibatches (default is True).\n",
    "\n",
    "    Returns:\n",
    "    - A generator that yields minibatches of data.\n",
    "    '''\n",
    "    assert x.shape[0] == y.shape[0], 'Error en cantidad de muestras'\n",
    "    total_data = x.shape[0]  # Total number of data samples\n",
    "    \n",
    "    if shuffle:\n",
    "        idxs = np.arange(total_data)  # Create an array of indices from 0 to total_data\n",
    "        np.random.shuffle(idxs)      # Shuffle the indices\n",
    "        x = x[idxs]                  # Shuffle the input features based on shuffled indices\n",
    "        y = y[idxs]                  # Shuffle the target labels based on shuffled indices\n",
    "    \n",
    "    # Create and return a generator that yields minibatches\n",
    "    return ((x[i:i+mb_size], y[i:i+mb_size]) for i in range(0, total_data, mb_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class np_tensor(np.ndarray): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuestra clase Linear, ReLU y Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Clase Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Initialize parameters using Kaiming He initialization.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size: Number of input features.\n",
    "        - output_size: Number of output features (neurons).\n",
    "        '''\n",
    "        # Initialize weights (self.W) using Kaiming He initialization\n",
    "        self.W = (np.random.randn(output_size, input_size) / np.sqrt(input_size/2)).view(np_tensor)\n",
    "        \n",
    "        # Initialize biases (self.b) as zeros\n",
    "        self.b = (np.zeros((output_size, 1))).view(np_tensor)\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        '''\n",
    "        Forward pass of the linear layer.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data.\n",
    "\n",
    "        Returns:\n",
    "        - Z: The linear transformation result.\n",
    "        '''\n",
    "        Z = self.W @ X + self.b  # Compute the linear transformation\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, X, Z):\n",
    "        '''\n",
    "        Backward pass of the linear layer.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data.\n",
    "        - Z: Linear transformation result.\n",
    "\n",
    "        Performs backpropagation and updates gradients.\n",
    "        '''\n",
    "        # Compute the gradient of the input data\n",
    "        X.grad = self.W.T @ Z.grad\n",
    "        \n",
    "        # Compute the gradient of the weights\n",
    "        self.W.grad = Z.grad @ X.T\n",
    "        \n",
    "        # Compute the gradient of the biases\n",
    "        self.b.grad = np.sum(Z.grad, axis = 1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __call__(self, Z):\n",
    "        '''\n",
    "        Forward pass of the Rectified Linear Unit (ReLU) activation function.\n",
    "        '''\n",
    "        A = np.maximum(0, Z)  # Compute the ReLU activation function\n",
    "        return A\n",
    "    \n",
    "    def backward(self, Z, A):\n",
    "        '''\n",
    "        Backward pass of the Rectified Linear Unit (ReLU) activation function.\n",
    "        Performs backpropagation and updates gradients.\n",
    "        '''\n",
    "        Z.grad = A.grad.copy()  # Copy the gradient from the output\n",
    "        Z.grad[Z <= 0] = 0      # Zero out the gradient where input was <= 0 (derivative of ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential_layers():\n",
    "    def __init__(self, layers):\n",
    "        '''\n",
    "        Initialize a Sequential neural network model with a list of layers.\n",
    "\n",
    "        Parameters:\n",
    "        - layers: A list containing objects of types Linear and ReLU.\n",
    "        '''\n",
    "        self.layers = layers  # Store the list of layers\n",
    "        self.x = None         # Initialize input data (x) to None\n",
    "        self.outputs = {}     # Initialize a dictionary to store intermediate outputs\n",
    "\n",
    "    def __call__(self, X):\n",
    "        '''\n",
    "        Forward pass of the Sequential neural network model.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data.\n",
    "\n",
    "        Returns:\n",
    "        - The final output of the model.\n",
    "        '''\n",
    "        self.x = X \n",
    "        self.outputs['l0'] = self.x  # Store the input data as 'l0' output\n",
    "        for i, layer in enumerate(self.layers, 1):\n",
    "            self.x = layer(self.x)  # Apply each layer in sequence\n",
    "            self.outputs['l'+str(i)]=self.x  # Store intermediate outputs\n",
    "        return self.x\n",
    "\n",
    "    def backward(self):\n",
    "        '''\n",
    "        Backward pass for the Sequential neural network model.\n",
    "\n",
    "        Performs backpropagation through the layers and updates gradients.\n",
    "        '''\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            self.layers[i].backward(self.outputs['l'+str(i)], self.outputs['l'+str(i+1)])\n",
    "\n",
    "    def update(self, learning_rate = 1e-3):\n",
    "        '''\n",
    "        Update the model's parameters using gradient descent.\n",
    "\n",
    "        Parameters:\n",
    "        - learning_rate: Learning rate for gradient descent (default is 1e-3).\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ReLU): continue  # Skip ReLU layers\n",
    "            layer.W = layer.W - learning_rate * layer.W.grad  # Update weights\n",
    "            layer.b = layer.b - learning_rate * layer.b.grad  # Update biases\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Make predictions using the model.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data.\n",
    "\n",
    "        Returns:\n",
    "        - The predicted class label.\n",
    "        '''\n",
    "        return np.argmax(self.__call__(X))  # Return the class with the highest output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxXEntropy(x, y):\n",
    "    '''\n",
    "    Compute the softmax cross-entropy loss and gradients.\n",
    "\n",
    "    Parameters:\n",
    "    - x: Input scores or logits.\n",
    "    - y: Target labels.\n",
    "\n",
    "    Returns:\n",
    "    - preds: Predicted probabilities.\n",
    "    - cost: Softmax cross-entropy loss.\n",
    "    '''\n",
    "    batch_size = x.shape[1]  # Get the batch size\n",
    "    \n",
    "    # Compute the softmax probabilities for each class\n",
    "    exp_scores = np.exp(x)\n",
    "    probs = exp_scores / exp_scores.sum(axis = 0)\n",
    "    preds = probs.copy()  # Store the predicted probabilities\n",
    "    \n",
    "    # Compute the softmax cross-entropy loss\n",
    "    y_hat = probs[y.squeeze(), np.arange(batch_size)]\n",
    "    cost = np.sum(-np.log(y_hat)) / batch_size\n",
    "    \n",
    "    # Calculate gradients for backpropagation\n",
    "    probs[y.squeeze(), np.arange(batch_size)] -= 1  # dL/dx\n",
    "    x.grad = probs.copy()  # Store gradients\n",
    "    \n",
    "    return preds, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x, y, mb_size, model):\n",
    "    '''\n",
    "    Compute the accuracy of a model on a given dataset.\n",
    "    '''\n",
    "    correct = 0  # Initialize the count of correctly predicted samples\n",
    "    total = 0    # Initialize the total number of samples\n",
    "    \n",
    "    # Iterate through mini-batches of data\n",
    "    for i, (x, y) in enumerate(create_minibatches(mb_size, x, y)):\n",
    "        # Make predictions using the model\n",
    "        pred = model(x.T.view(np_tensor))\n",
    "        \n",
    "        # Count the number of correctly predicted samples in the mini-batch\n",
    "        correct += np.sum(np.argmax(pred, axis=0) == y.squeeze())\n",
    "        \n",
    "        # Update the total number of samples\n",
    "        total += pred.shape[1]\n",
    "    \n",
    "    # Calculate and return the accuracy\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27455, 784)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, mb_size=128, learning_rate=1e-3):\n",
    "    '''\n",
    "    Train a neural network model using mini-batch gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The neural network model to be trained.\n",
    "    - epochs: Number of training epochs.\n",
    "    - mb_size: Mini-batch size for training (default is 128).\n",
    "    - learning_rate: Learning rate for gradient descent (default is 1e-3).\n",
    "    '''\n",
    "    for epoch in range(epochs):\n",
    "        for i, (x, y) in enumerate(create_minibatches(mb_size, x_train, y_train)):\n",
    "            # Forward pass to compute scores and cost\n",
    "            scores = model(x.T.view(np_tensor))\n",
    "            _, cost = softmaxXEntropy(scores, y)\n",
    "            \n",
    "            # Backward pass for gradient computation\n",
    "            model.backward()\n",
    "            \n",
    "            # Update model parameters using gradient descent\n",
    "            model.update(learning_rate)\n",
    "        \n",
    "        # Compute and print cost and accuracy for the current epoch\n",
    "        print(f'costo: {cost}, accuracy: {accuracy(x_val, y_val, mb_size, model)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default\n",
    "#model = Sequential_layers([Linear(784, 200), ReLU(), Linear(200, 200), ReLU(), Linear(200, 10)])\n",
    "#mb_size = 512\n",
    "#learning_rate = 1e-4\n",
    "#epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network model with a sequential architecture\n",
    "model = Sequential_layers([\n",
    "    Linear(784, 400), ReLU(),       # Layer 1: 784 input features, 400 output features with ReLU activation\n",
    "    Linear(400, 400), ReLU(),       # Layer 2: 400 input features, 400 output features with ReLU activation\n",
    "    Linear(400, 400), ReLU(),       # Layer 3: 400 input features, 400 output features with ReLU activation\n",
    "    Linear(400, 400), ReLU(),       # Layer 4: 400 input features, 400 output features with ReLU activation\n",
    "    Linear(400, 400), ReLU(),       # Layer 5: 400 input features, 400 output features with ReLU activation\n",
    "    Linear(400, 24)                 # Layer 6: 400 input features, 24 output features\n",
    "])\n",
    "\n",
    "# Set mini-batch size, learning rate, and number of training epochs\n",
    "mb_size = 512           # Mini-batch size\n",
    "learning_rate = 1e-4    # Learning rate for gradient descent\n",
    "epochs = 35             # Number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "costo: 0.634166192010805, accuracy: 0.6318847315826168\n",
      "costo: 0.18139213342227714, accuracy: 0.71136416453637\n",
      "costo: 0.06596980455177855, accuracy: 0.7483151289797816\n",
      "costo: 0.02932712813813073, accuracy: 0.7571461770857542\n",
      "costo: 0.022560870013927935, accuracy: 0.7592377411108528\n",
      "costo: 0.01323260620348495, accuracy: 0.7636532651638391\n",
      "costo: 0.013260785772818385, accuracy: 0.7652800371833605\n",
      "costo: 0.009084816384830429, accuracy: 0.7645828491749942\n",
      "costo: 0.00679910917369437, accuracy: 0.7659772251917267\n",
      "costo: 0.004989895576313538, accuracy: 0.7683011852196142\n",
      "costo: 0.0043344932261434954, accuracy: 0.7692307692307693\n",
      "costo: 0.004034431241239863, accuracy: 0.7662096211945154\n",
      "costo: 0.0034726962440427603, accuracy: 0.768533581222403\n",
      "costo: 0.004227608894213869, accuracy: 0.7692307692307693\n",
      "costo: 0.00322969089907983, accuracy: 0.770392749244713\n",
      "costo: 0.0028312816268371973, accuracy: 0.7689983732279805\n",
      "costo: 0.0028232328380122275, accuracy: 0.7717871252614456\n",
      "costo: 0.0026083998810527997, accuracy: 0.774111085289333\n",
      "costo: 0.002061563029297768, accuracy: 0.7748082732976993\n",
      "costo: 0.0022482590505902866, accuracy: 0.7715547292586568\n",
      "costo: 0.0019849888905950175, accuracy: 0.7766674413200093\n",
      "costo: 0.0019388089177668963, accuracy: 0.775970253311643\n",
      "costo: 0.0019379702760566255, accuracy: 0.7757378573088543\n",
      "costo: 0.0015187262297825415, accuracy: 0.7755054613060656\n",
      "costo: 0.0017019645619975445, accuracy: 0.7766674413200093\n",
      "costo: 0.0013595734624226065, accuracy: 0.7764350453172205\n",
      "costo: 0.0017758785080941204, accuracy: 0.7764350453172205\n",
      "costo: 0.0011874207301669792, accuracy: 0.7780618173367418\n",
      "costo: 0.0016423610873137443, accuracy: 0.7764350453172205\n",
      "costo: 0.0012317835661218319, accuracy: 0.776899837322798\n",
      "costo: 0.001430282788166244, accuracy: 0.7775970253311643\n",
      "costo: 0.001182591440094418, accuracy: 0.7773646293283756\n",
      "costo: 0.001161590618493477, accuracy: 0.7780618173367418\n",
      "costo: 0.0008809914953656594, accuracy: 0.7782942133395305\n",
      "costo: 0.0009517274219970884, accuracy: 0.7789914013478968\n"
     ]
    }
   ],
   "source": [
    "train(model, epochs, mb_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your model on Random data from your test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "### Alphabet array to determine results\n",
    "\n",
    "alphabet=list(string.ascii_lowercase)\n",
    "alphabet.remove('j') # Not included in the photos\n",
    "alphabet.remove('z')\n",
    "print(len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el valor predicho es: n el valor real es:n\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(len(y_test))\n",
    "plot_number(x_test[idx].reshape(28,28))\n",
    "pred = model.predict(x_test[idx].reshape(-1, 1))\n",
    "print(f'el valor predicho es: {alphabet[pred]} el valor real es:{alphabet[y_test[idx]]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
